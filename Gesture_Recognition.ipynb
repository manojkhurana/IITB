{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVsfU7c88lAy"
   },
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eZCMTG3r8lA3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import imageio\n",
    "from imageio import imread\n",
    "import cv2\n",
    "\n",
    "def imread(path):\n",
    "    from PIL import Image\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "def imresize(img, size):\n",
    "    from PIL import Image\n",
    "    return np.array(Image.fromarray(img).resize(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UqBX3jT8lA5"
   },
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gvll674A8lA6"
   },
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iQMkIPN8lA7"
   },
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NlarN-f-8lA7"
   },
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "batch_size = 16 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6B0XmmC8lA9"
   },
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "X5ieDy7Q8lA-"
   },
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, img_idx_v,img_h, img_w):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = img_idx_v  #list of image numbers you want to use for a particular video passed to generator\n",
    "    y = img_h # image height we want to use with model passsed to generator\n",
    "    z = img_w # image width we want to use with model passsed to generator\n",
    "    x = len(img_idx)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(folder_list)/batch_size) # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            #x=len(img_idx)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3))  # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    height, width , channel = image.shape\n",
    "                    if height == 120 or width == 120:\n",
    "                        image=image[20:140,:120,:]\n",
    "                    \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = cv2.resize(image,(y,z))\n",
    "                    image_r = image\n",
    "                    image_r[:,:,1:2] = 0\n",
    "                    batch_data[folder,idx,:,:,0] = (image_r[:,:,0] - image_r.mean())/image_r.std()#normalise and feed in the image\n",
    "                    \n",
    "                    image_g = image\n",
    "                    image_g[:,:,0:2] = 0\n",
    "                    batch_data[folder,idx,:,:,1] = (image_g[:,:,1] - image_g.mean())/image_g.std()#normalise and feed in the image\n",
    "                    \n",
    "                    image_b = image\n",
    "                    image_b[:,:,0:1] = 0\n",
    "                    batch_data[folder,idx,:,:,2] = (image_b[:,:,2] - image_b.mean())/image_b.std() #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        \n",
    "        batch_size_r=len(folder_list)%batch_size\n",
    "        if batch_size_r!=0:\n",
    "            batch_data = np.zeros((batch_size_r,x,y,z,3))\n",
    "            batch_labels = np.zeros((batch_size_r,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size_r): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    height, width , channel = image.shape\n",
    "                    if height == 120 or width == 120:\n",
    "                        image=image[20:140,:120,:]\n",
    "                    \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = cv2.resize(image,(y,z))\n",
    "                    image_r = image\n",
    "                    image_r[:,:,1:2] = 0\n",
    "                    batch_data[folder,idx,:,:,0] = (image_r[:,:,0] - image_r.mean())/image_r.std()#normalise and feed in the image\n",
    "                    \n",
    "                    image_g = image\n",
    "                    image_g[:,:,0:2] = 0\n",
    "                    batch_data[folder,idx,:,:,1] = (image_g[:,:,1] - image_g.mean())/image_g.std()#normalise and feed in the image\n",
    "                    \n",
    "                    image_b = image\n",
    "                    image_b[:,:,0:1] = 0\n",
    "                    batch_data[folder,idx,:,:,2] = (image_b[:,:,2] - image_b.mean())/image_b.std() #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjpF1iBY8lBA"
   },
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOI-XCAH8lBB",
    "outputId": "b97ee3e9-b7b0-4eea-ff6d-07429d2fabe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 15\n"
     ]
    }
   ],
   "source": [
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 15 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR8tmrjd8lBC"
   },
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "###\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, Conv2D, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60gDBeOsvzGx"
   },
   "source": [
    "#### We will experiment different conv3D models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model with Limited Data\n",
    "### Model 1 Batch Size: 128, Number of images from each video: 15 Image Size: 160x160, Epochs: 15 - Tried with batch size 128 but resources exhausted\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Batch Size to 64\n",
    "### Model 2 Batch Size: 64, Number of images from each video: 15 Image Size: 160x160, Epochs: 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 15, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 15, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 8, 80, 80, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 8, 80, 80, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 8, 80, 80, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 80, 80, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 4, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 4, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 2, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 2, 20, 20, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1638528   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,932,485\n",
      "Trainable params: 1,931,749\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "img_per_video=15\n",
    "img_h = 160\n",
    "img_w = 160\n",
    "batch_size = 64#experiment with the batch size\n",
    "filtersize = (3,3,3)\n",
    "img_idx_v = np.round(np.linspace(0,29,img_per_video)).astype(int)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(16, filtersize, activation='relu', input_shape=(len(img_idx_v),img_h,img_w,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "\n",
    "model.add(Conv3D(32, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "\n",
    "model.add(Conv3D(128, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "optimiser = 'adam' #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t71rWTyKIQOs"
   },
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_idx_v, img_h, img_w)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_idx_v, img_h, img_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.001)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjZ2Vy0R8lBL"
   },
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 64\n",
      "Epoch 1/15\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.7425 - categorical_accuracy: 0.3843Source path =  Project_data/val ; batch size = 64\n",
      "11/11 [==============================] - 133s 11s/step - loss: 1.7166 - categorical_accuracy: 0.3917 - val_loss: 5.4689 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.46895, saving model to model_init_2021-11-1813_02_42.391809/model-00001-1.43153-0.47210-5.46895-0.21000.h5\n",
      "Epoch 2/15\n",
      "11/11 [==============================] - 96s 10s/step - loss: 0.7025 - categorical_accuracy: 0.7739 - val_loss: 2.5107 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.46895 to 2.51069, saving model to model_init_2021-11-1813_02_42.391809/model-00002-0.67901-0.77979-2.51069-0.24000.h5\n",
      "Epoch 3/15\n",
      "11/11 [==============================] - 81s 8s/step - loss: 0.4033 - categorical_accuracy: 0.8642 - val_loss: 1.4857 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.51069 to 1.48566, saving model to model_init_2021-11-1813_02_42.391809/model-00003-0.38799-0.87330-1.48566-0.32000.h5\n",
      "Epoch 4/15\n",
      "11/11 [==============================] - 59s 6s/step - loss: 0.2179 - categorical_accuracy: 0.9368 - val_loss: 2.1649 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.48566\n",
      "Epoch 5/15\n",
      "11/11 [==============================] - 56s 5s/step - loss: 0.1509 - categorical_accuracy: 0.9676 - val_loss: 2.6087 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.48566\n",
      "Epoch 6/15\n",
      "11/11 [==============================] - 54s 5s/step - loss: 0.0828 - categorical_accuracy: 0.9857 - val_loss: 2.8973 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.48566\n",
      "Epoch 7/15\n",
      "11/11 [==============================] - 52s 5s/step - loss: 0.0351 - categorical_accuracy: 0.9973 - val_loss: 3.2484 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.48566\n",
      "Epoch 8/15\n",
      "11/11 [==============================] - 50s 5s/step - loss: 0.0252 - categorical_accuracy: 0.9983 - val_loss: 3.6776 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.48566\n",
      "Epoch 9/15\n",
      "11/11 [==============================] - 51s 5s/step - loss: 0.0176 - categorical_accuracy: 0.9996 - val_loss: 3.9900 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.48566\n",
      "Epoch 10/15\n",
      "11/11 [==============================] - 48s 5s/step - loss: 0.0148 - categorical_accuracy: 0.9984 - val_loss: 3.5683 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.48566\n",
      "Epoch 11/15\n",
      "11/11 [==============================] - 49s 5s/step - loss: 0.0113 - categorical_accuracy: 1.0000 - val_loss: 3.9528 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.48566\n",
      "Epoch 12/15\n",
      "11/11 [==============================] - 47s 5s/step - loss: 0.0062 - categorical_accuracy: 1.0000 - val_loss: 4.1638 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.48566\n",
      "Epoch 13/15\n",
      "11/11 [==============================] - 49s 5s/step - loss: 0.0048 - categorical_accuracy: 1.0000 - val_loss: 4.1603 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.48566\n",
      "Epoch 14/15\n",
      "11/11 [==============================] - 48s 5s/step - loss: 0.0041 - categorical_accuracy: 1.0000 - val_loss: 4.4990 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.48566\n",
      "Epoch 15/15\n",
      "11/11 [==============================] - 49s 5s/step - loss: 0.0035 - categorical_accuracy: 1.0000 - val_loss: 4.1710 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.48566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2f6401cc40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see a clear overfit model. Let us try to add Dropouts\n",
    "### Model 3 Batch Size: 64, Number of images from each video: 15 Image Size: 160x160, Epochs: 15 with Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 15, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 15, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 8, 80, 80, 16)     0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 80, 80, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 8, 80, 80, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 8, 80, 80, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 80, 80, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 4, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 4, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 2, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 2, 20, 20, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1638528   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,932,485\n",
      "Trainable params: 1,931,749\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "img_per_video=15\n",
    "img_h = 160\n",
    "img_w = 160\n",
    "batch_size = 64 #experiment with the batch size\n",
    "filtersize = (3,3,3)\n",
    "img_idx_v = np.round(np.linspace(0,29,img_per_video)).astype(int)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(16, filtersize, activation='relu', input_shape=(len(img_idx_v),img_h,img_w,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv3D(32, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(128, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "optimiser = 'adam' #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_idx_v, img_h, img_w)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_idx_v, img_h, img_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.001)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 64\n",
      "Epoch 1/15\n",
      "11/11 [==============================] - ETA: 0s - loss: 2.1626 - categorical_accuracy: 0.2936Source path =  Project_data/val ; batch size = 64\n",
      "11/11 [==============================] - 130s 11s/step - loss: 2.1407 - categorical_accuracy: 0.2983 - val_loss: 1.6377 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.63770, saving model to model_init_2021-11-1813_40_53.447876/model-00001-1.89984-0.34992-1.63770-0.39000.h5\n",
      "Epoch 2/15\n",
      "11/11 [==============================] - 85s 8s/step - loss: 1.3423 - categorical_accuracy: 0.4642 - val_loss: 1.7286 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.63770\n",
      "Epoch 3/15\n",
      "11/11 [==============================] - 77s 8s/step - loss: 1.1460 - categorical_accuracy: 0.5586 - val_loss: 2.3025 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.63770\n",
      "Epoch 4/15\n",
      "11/11 [==============================] - 57s 6s/step - loss: 0.9989 - categorical_accuracy: 0.6072 - val_loss: 4.7872 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.63770\n",
      "Epoch 5/15\n",
      "11/11 [==============================] - 52s 5s/step - loss: 0.7754 - categorical_accuracy: 0.6977 - val_loss: 4.5788 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.63770\n",
      "Epoch 6/15\n",
      "11/11 [==============================] - 48s 5s/step - loss: 0.5919 - categorical_accuracy: 0.7875 - val_loss: 4.5946 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.63770\n",
      "Epoch 7/15\n",
      "11/11 [==============================] - 49s 5s/step - loss: 0.4662 - categorical_accuracy: 0.8296 - val_loss: 3.8523 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.63770\n",
      "Epoch 8/15\n",
      "11/11 [==============================] - 47s 5s/step - loss: 0.4492 - categorical_accuracy: 0.8325 - val_loss: 3.2740 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.63770\n",
      "Epoch 9/15\n",
      "11/11 [==============================] - 49s 5s/step - loss: 0.3442 - categorical_accuracy: 0.9021 - val_loss: 3.2903 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.63770\n",
      "Epoch 10/15\n",
      "11/11 [==============================] - 48s 5s/step - loss: 0.2925 - categorical_accuracy: 0.9008 - val_loss: 3.0693 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.63770\n",
      "Epoch 11/15\n",
      "11/11 [==============================] - 49s 5s/step - loss: 0.2407 - categorical_accuracy: 0.9222 - val_loss: 3.5752 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.63770\n",
      "Epoch 12/15\n",
      "11/11 [==============================] - 47s 5s/step - loss: 0.2205 - categorical_accuracy: 0.9279 - val_loss: 3.6842 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.63770\n",
      "Epoch 13/15\n",
      "11/11 [==============================] - 49s 5s/step - loss: 0.1871 - categorical_accuracy: 0.9485 - val_loss: 3.3801 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.63770\n",
      "Epoch 14/15\n",
      "11/11 [==============================] - 47s 5s/step - loss: 0.1618 - categorical_accuracy: 0.9442 - val_loss: 3.7081 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.63770\n",
      "Epoch 15/15\n",
      "11/11 [==============================] - 49s 5s/step - loss: 0.1092 - categorical_accuracy: 0.9760 - val_loss: 3.7738 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.63770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff44c4d53d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHvlRNbiAw-k"
   },
   "source": [
    "#### Dropouts have not helped that much. Let us add more data and try with 22 images per video\n",
    "\n",
    "### Model 4 Batch Size: 64, Number of images from each video: 22 Image Size: 160x160, Epochs: 15 with Dropouts\n",
    "\n",
    "#### Tried But Kernel Died so Reducing the batch Size in next model to 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 5 Batch Size: 40, Number of images from each video: 22 Image Size: 160x160, Epochs: 15 with Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vn_-2ClA8lBD",
    "outputId": "4bf37a4a-7c87-455f-cb39-5a9959bde60b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_4 (Conv3D)            (None, 22, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 22, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 11, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 11, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 11, 80, 80, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 11, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 11, 80, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 5, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 5, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 5, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 3, 20, 20, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 3, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 2, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               3276928   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 3,570,885\n",
      "Trainable params: 3,570,149\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "img_per_video=22\n",
    "img_h = 160\n",
    "img_w = 160\n",
    "batch_size = 40 #experiment with the batch size\n",
    "filtersize = (3,3,3)\n",
    "img_idx_v = np.round(np.linspace(0,29,img_per_video)).astype(int)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(16, filtersize, activation='relu', input_shape=(len(img_idx_v),img_h,img_w,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv3D(32, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(128, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "optimiser = 'adam' #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_idx_v, img_h, img_w)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_idx_v, img_h, img_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.001)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 40\n",
      "Epoch 1/15\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.9767 - categorical_accuracy: 0.3205Source path =  Project_data/val ; batch size = 40\n",
      "17/17 [==============================] - 167s 9s/step - loss: 1.9617 - categorical_accuracy: 0.3243 - val_loss: 1.7782 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.77818, saving model to model_init_2021-11-1814_01_50.031547/model-00001-1.70793-0.38763-1.77818-0.33000.h5\n",
      "Epoch 2/15\n",
      "17/17 [==============================] - 124s 8s/step - loss: 1.0617 - categorical_accuracy: 0.5921 - val_loss: 2.3478 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.77818\n",
      "Epoch 3/15\n",
      "17/17 [==============================] - 91s 6s/step - loss: 0.8193 - categorical_accuracy: 0.6873 - val_loss: 3.6083 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.77818\n",
      "Epoch 4/15\n",
      "17/17 [==============================] - 77s 5s/step - loss: 0.6520 - categorical_accuracy: 0.7546 - val_loss: 2.6585 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.77818\n",
      "Epoch 5/15\n",
      "17/17 [==============================] - 84s 5s/step - loss: 0.4849 - categorical_accuracy: 0.8270 - val_loss: 3.1019 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.77818\n",
      "Epoch 6/15\n",
      "17/17 [==============================] - 74s 5s/step - loss: 0.3823 - categorical_accuracy: 0.8827 - val_loss: 3.2522 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.77818\n",
      "Epoch 7/15\n",
      "17/17 [==============================] - 75s 5s/step - loss: 0.3291 - categorical_accuracy: 0.8826 - val_loss: 2.8327 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.77818\n",
      "Epoch 8/15\n",
      "17/17 [==============================] - 73s 4s/step - loss: 0.2597 - categorical_accuracy: 0.9173 - val_loss: 3.4197 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.77818\n",
      "Epoch 9/15\n",
      "17/17 [==============================] - 72s 4s/step - loss: 0.1807 - categorical_accuracy: 0.9478 - val_loss: 3.0417 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.77818\n",
      "Epoch 10/15\n",
      "17/17 [==============================] - 75s 5s/step - loss: 0.1734 - categorical_accuracy: 0.9613 - val_loss: 3.0885 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.77818\n",
      "Epoch 11/15\n",
      "17/17 [==============================] - 74s 5s/step - loss: 0.1315 - categorical_accuracy: 0.9697 - val_loss: 3.0238 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.77818\n",
      "Epoch 12/15\n",
      "17/17 [==============================] - 74s 5s/step - loss: 0.1083 - categorical_accuracy: 0.9766 - val_loss: 2.3040 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.77818\n",
      "Epoch 13/15\n",
      "17/17 [==============================] - 76s 5s/step - loss: 0.0995 - categorical_accuracy: 0.9854 - val_loss: 2.8797 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.77818\n",
      "Epoch 14/15\n",
      "17/17 [==============================] - 76s 5s/step - loss: 0.0704 - categorical_accuracy: 0.9943 - val_loss: 2.6076 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.77818\n",
      "Epoch 15/15\n",
      "17/17 [==============================] - 74s 5s/step - loss: 0.0732 - categorical_accuracy: 0.9906 - val_loss: 3.3223 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.77818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f29fc0c40a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Though we saw a little better model but still its overfitting.  Let us try to simplify by making change in dense layer\n",
    "\n",
    "### Model 6 Batch Size: 40, Number of images from each video: 22 Image Size: 160x160, Epochs: 15 with Dropouts, simplied dense layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_4 (Conv3D)            (None, 22, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 22, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 11, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 11, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 11, 80, 80, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 11, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 11, 80, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 5, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 5, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 5, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 3, 20, 20, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 3, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 2, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                1638464   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,931,845\n",
      "Trainable params: 1,931,237\n",
      "Non-trainable params: 608\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "img_per_video=22\n",
    "img_h = 160\n",
    "img_w = 160\n",
    "batch_size = 40 #experiment with the batch size\n",
    "filtersize = (3,3,3)\n",
    "img_idx_v = np.round(np.linspace(0,29,img_per_video)).astype(int)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(16, filtersize, activation='relu', input_shape=(len(img_idx_v),img_h,img_w,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv3D(32, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(128, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "optimiser = 'adam' #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_idx_v, img_h, img_w)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_idx_v, img_h, img_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.001)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 40\n",
      "Epoch 1/15\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.1850 - categorical_accuracy: 0.3023Source path =  Project_data/val ; batch size = 40\n",
      "17/17 [==============================] - 171s 9s/step - loss: 2.1656 - categorical_accuracy: 0.3061 - val_loss: 1.5208 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.52083, saving model to model_init_2021-11-1900_37_59.296895/model-00001-1.83561-0.37104-1.52083-0.28000.h5\n",
      "Epoch 2/15\n",
      "17/17 [==============================] - 147s 9s/step - loss: 1.2668 - categorical_accuracy: 0.5312 - val_loss: 2.2683 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.52083\n",
      "Epoch 3/15\n",
      "17/17 [==============================] - 129s 8s/step - loss: 0.9988 - categorical_accuracy: 0.6130 - val_loss: 2.6391 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.52083\n",
      "Epoch 4/15\n",
      "17/17 [==============================] - 105s 6s/step - loss: 0.9182 - categorical_accuracy: 0.6315 - val_loss: 3.4139 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.52083\n",
      "Epoch 5/15\n",
      "17/17 [==============================] - 103s 6s/step - loss: 0.6712 - categorical_accuracy: 0.7493 - val_loss: 3.3873 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.52083\n",
      "Epoch 6/15\n",
      "17/17 [==============================] - 109s 7s/step - loss: 0.6205 - categorical_accuracy: 0.7867 - val_loss: 3.6415 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.52083\n",
      "Epoch 7/15\n",
      "17/17 [==============================] - 104s 6s/step - loss: 0.4814 - categorical_accuracy: 0.8214 - val_loss: 4.0738 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.52083\n",
      "Epoch 8/15\n",
      "17/17 [==============================] - 100s 6s/step - loss: 0.4208 - categorical_accuracy: 0.8659 - val_loss: 3.9716 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.52083\n",
      "Epoch 9/15\n",
      "17/17 [==============================] - 95s 6s/step - loss: 0.3007 - categorical_accuracy: 0.9131 - val_loss: 3.8853 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.52083\n",
      "Epoch 10/15\n",
      "17/17 [==============================] - 93s 6s/step - loss: 0.2617 - categorical_accuracy: 0.9340 - val_loss: 4.0210 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.52083\n",
      "Epoch 11/15\n",
      "17/17 [==============================] - 96s 6s/step - loss: 0.2048 - categorical_accuracy: 0.9516 - val_loss: 3.6322 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.52083\n",
      "Epoch 12/15\n",
      "17/17 [==============================] - 94s 6s/step - loss: 0.1712 - categorical_accuracy: 0.9587 - val_loss: 3.2778 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.52083\n",
      "Epoch 13/15\n",
      "17/17 [==============================] - 92s 6s/step - loss: 0.1476 - categorical_accuracy: 0.9716 - val_loss: 3.4811 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.52083\n",
      "Epoch 14/15\n",
      "17/17 [==============================] - 94s 6s/step - loss: 0.1039 - categorical_accuracy: 0.9872 - val_loss: 3.7822 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.52083\n",
      "Epoch 15/15\n",
      "17/17 [==============================] - 89s 5s/step - loss: 0.1218 - categorical_accuracy: 0.9788 - val_loss: 4.0159 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.52083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd390327dc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with reduced batch size, lower LR , additional dense layer and more Epochs\n",
    "### Model 7 Batch Size: 20, Number of images from each video: 22 Image Size: 160x160, Epochs: 25 with Dropouts, two dense layer, LR=0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 22, 160, 160, 16)  1312      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 22, 160, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 11, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 11, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 11, 80, 80, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 11, 80, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 11, 80, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 5, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 5, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 5, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 3, 20, 20, 128)    221312    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3, 20, 20, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 20, 20, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 2, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 10, 10, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1638464   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,936,261\n",
      "Trainable params: 1,935,525\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "img_per_video=22\n",
    "img_h = 160\n",
    "img_w = 160\n",
    "batch_size = 20 #experiment with the batch size\n",
    "filtersize = (3,3,3)\n",
    "num_epochs = 25 # choose the number of epochs\n",
    "img_idx_v = np.round(np.linspace(0,29,img_per_video)).astype(int)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(16, filtersize, activation='relu', input_shape=(len(img_idx_v),img_h,img_w,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv3D(32, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(128, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam(lr=0.0002) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_idx_v, img_h, img_w)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_idx_v, img_h, img_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.001)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.4417 - categorical_accuracy: 0.2325Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 171s 5s/step - loss: 2.4399 - categorical_accuracy: 0.2332 - val_loss: 2.8917 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.89170, saving model to model_init_2021-11-1901_34_29.620632/model-00001-2.37912-0.25490-2.89170-0.16000.h5\n",
      "Epoch 2/25\n",
      "34/34 [==============================] - 70s 2s/step - loss: 1.8623 - categorical_accuracy: 0.3272 - val_loss: 6.3100 - val_categorical_accuracy: 0.1300\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.89170\n",
      "Epoch 3/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.9954 - categorical_accuracy: 0.2580 - val_loss: 7.2578 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.89170\n",
      "Epoch 4/25\n",
      "34/34 [==============================] - 71s 2s/step - loss: 1.7600 - categorical_accuracy: 0.3821 - val_loss: 7.5202 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.89170\n",
      "Epoch 5/25\n",
      "34/34 [==============================] - 70s 2s/step - loss: 1.5668 - categorical_accuracy: 0.4219 - val_loss: 6.0523 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.89170\n",
      "Epoch 6/25\n",
      "34/34 [==============================] - 70s 2s/step - loss: 1.4844 - categorical_accuracy: 0.4840 - val_loss: 6.0954 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.89170\n",
      "Epoch 7/25\n",
      "34/34 [==============================] - 70s 2s/step - loss: 1.4987 - categorical_accuracy: 0.4462 - val_loss: 5.3508 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.89170\n",
      "Epoch 8/25\n",
      "34/34 [==============================] - 70s 2s/step - loss: 1.3648 - categorical_accuracy: 0.5253 - val_loss: 5.1561 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.89170\n",
      "Epoch 9/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.3821 - categorical_accuracy: 0.5165 - val_loss: 5.4123 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.89170\n",
      "Epoch 10/25\n",
      "34/34 [==============================] - 70s 2s/step - loss: 1.1809 - categorical_accuracy: 0.5589 - val_loss: 8.0665 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.89170\n",
      "Epoch 11/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.3063 - categorical_accuracy: 0.5293 - val_loss: 6.5829 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.89170\n",
      "Epoch 12/25\n",
      "34/34 [==============================] - 70s 2s/step - loss: 1.3431 - categorical_accuracy: 0.4923 - val_loss: 5.5970 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.89170\n",
      "Epoch 13/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.2224 - categorical_accuracy: 0.5252 - val_loss: 5.9168 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.89170\n",
      "Epoch 14/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.1297 - categorical_accuracy: 0.5619 - val_loss: 10.0363 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.89170\n",
      "Epoch 15/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.0949 - categorical_accuracy: 0.5602 - val_loss: 10.9477 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.89170\n",
      "Epoch 16/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 0.9905 - categorical_accuracy: 0.5925 - val_loss: 8.3734 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.89170\n",
      "Epoch 17/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.0193 - categorical_accuracy: 0.6146 - val_loss: 7.7244 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.89170\n",
      "Epoch 18/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.0139 - categorical_accuracy: 0.6316 - val_loss: 5.2207 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.89170\n",
      "Epoch 19/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.0965 - categorical_accuracy: 0.5957 - val_loss: 3.5293 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.89170\n",
      "Epoch 20/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 0.9763 - categorical_accuracy: 0.6438 - val_loss: 3.6798 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.89170\n",
      "Epoch 21/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 1.0940 - categorical_accuracy: 0.6060 - val_loss: 2.2781 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.89170 to 2.27810, saving model to model_init_2021-11-1901_34_29.620632/model-00021-1.07977-0.59879-2.27810-0.39000.h5\n",
      "Epoch 22/25\n",
      "34/34 [==============================] - 68s 2s/step - loss: 0.9779 - categorical_accuracy: 0.6317 - val_loss: 2.2184 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.27810 to 2.21839, saving model to model_init_2021-11-1901_34_29.620632/model-00022-0.99488-0.62594-2.21839-0.45000.h5\n",
      "Epoch 23/25\n",
      "34/34 [==============================] - 70s 2s/step - loss: 0.9726 - categorical_accuracy: 0.6288 - val_loss: 2.0249 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.21839 to 2.02492, saving model to model_init_2021-11-1901_34_29.620632/model-00023-0.97412-0.64103-2.02492-0.46000.h5\n",
      "Epoch 24/25\n",
      "34/34 [==============================] - 69s 2s/step - loss: 0.9079 - categorical_accuracy: 0.6239 - val_loss: 2.2978 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.02492\n",
      "Epoch 25/25\n",
      "34/34 [==============================] - 70s 2s/step - loss: 0.8113 - categorical_accuracy: 0.6575 - val_loss: 3.4266 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.02492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7eff967acf70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with lower filter size and image size and more epochs, lower dropout after Conv3D and higher after Dense\n",
    "\n",
    "### Model 8 Batch Size: 20, Number of images from each video: 22 Image Size: 120x120, Epochs: 40 with Dropouts, two dense layer, LR=0.0002, filter size =(2,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_28 (Conv3D)           (None, 22, 120, 120, 16)  400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 22, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_28 (MaxPooling (None, 11, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 11, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_29 (Conv3D)           (None, 11, 60, 60, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 11, 60, 60, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 11, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_29 (MaxPooling (None, 5, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 5, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_30 (Conv3D)           (None, 5, 30, 30, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 5, 30, 30, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 5, 30, 30, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_30 (MaxPooling (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_31 (Conv3D)           (None, 2, 15, 15, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 2, 15, 15, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 2, 15, 15, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_31 (MaxPooling (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 1, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 908,725\n",
      "Trainable params: 907,733\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "img_per_video=22\n",
    "img_h = 120\n",
    "img_w = 120\n",
    "batch_size = 20 #experiment with the batch size\n",
    "filtersize = (2,2,2)\n",
    "num_epochs = 40 # choose the number of epochs\n",
    "img_idx_v = np.round(np.linspace(0,29,img_per_video)).astype(int)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(16, filtersize, activation='relu', input_shape=(len(img_idx_v),img_h,img_w,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(32, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(64, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(128, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam(lr=0.0002) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_idx_v, img_h, img_w)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_idx_v, img_h, img_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.001)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/40\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.5915 - categorical_accuracy: 0.2398Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 67s 2s/step - loss: 2.5865 - categorical_accuracy: 0.2407 - val_loss: 1.7102 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.71019, saving model to model_init_2021-11-1903_30_56.460227/model-00001-2.41480-0.26998-1.71019-0.16000.h5\n",
      "Epoch 2/40\n",
      "34/34 [==============================] - 91s 3s/step - loss: 2.0717 - categorical_accuracy: 0.3303 - val_loss: 2.2551 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.71019\n",
      "Epoch 3/40\n",
      "34/34 [==============================] - 82s 2s/step - loss: 1.8094 - categorical_accuracy: 0.3889 - val_loss: 2.8326 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.71019\n",
      "Epoch 4/40\n",
      "34/34 [==============================] - 88s 3s/step - loss: 1.7368 - categorical_accuracy: 0.4106 - val_loss: 2.8995 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.71019\n",
      "Epoch 5/40\n",
      "34/34 [==============================] - 85s 3s/step - loss: 1.4696 - categorical_accuracy: 0.4899 - val_loss: 3.3632 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.71019\n",
      "Epoch 6/40\n",
      "34/34 [==============================] - 88s 3s/step - loss: 1.1340 - categorical_accuracy: 0.5723 - val_loss: 3.4457 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.71019\n",
      "Epoch 7/40\n",
      "34/34 [==============================] - 87s 3s/step - loss: 1.1967 - categorical_accuracy: 0.5718 - val_loss: 3.5215 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.71019\n",
      "Epoch 8/40\n",
      "34/34 [==============================] - 90s 3s/step - loss: 1.2542 - categorical_accuracy: 0.5500 - val_loss: 2.9750 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.71019\n",
      "Epoch 9/40\n",
      "34/34 [==============================] - 87s 3s/step - loss: 1.1022 - categorical_accuracy: 0.6159 - val_loss: 2.6763 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.71019\n",
      "Epoch 10/40\n",
      "34/34 [==============================] - 92s 3s/step - loss: 1.0514 - categorical_accuracy: 0.5939 - val_loss: 2.5333 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.71019\n",
      "Epoch 11/40\n",
      "34/34 [==============================] - 92s 3s/step - loss: 1.0924 - categorical_accuracy: 0.6246 - val_loss: 2.3814 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.71019\n",
      "Epoch 12/40\n",
      "34/34 [==============================] - 97s 3s/step - loss: 1.0225 - categorical_accuracy: 0.6241 - val_loss: 1.9191 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.71019\n",
      "Epoch 13/40\n",
      "34/34 [==============================] - 90s 3s/step - loss: 0.9660 - categorical_accuracy: 0.6251 - val_loss: 1.9626 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.71019\n",
      "Epoch 14/40\n",
      "34/34 [==============================] - 92s 3s/step - loss: 0.9368 - categorical_accuracy: 0.6724 - val_loss: 1.4232 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.71019 to 1.42318, saving model to model_init_2021-11-1903_30_56.460227/model-00014-0.90126-0.68024-1.42318-0.50000.h5\n",
      "Epoch 15/40\n",
      "34/34 [==============================] - 99s 3s/step - loss: 0.7785 - categorical_accuracy: 0.7009 - val_loss: 1.3082 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.42318 to 1.30819, saving model to model_init_2021-11-1903_30_56.460227/model-00015-0.85795-0.67873-1.30819-0.52000.h5\n",
      "Epoch 16/40\n",
      "34/34 [==============================] - 94s 3s/step - loss: 0.9854 - categorical_accuracy: 0.6189 - val_loss: 1.3013 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.30819 to 1.30130, saving model to model_init_2021-11-1903_30_56.460227/model-00016-0.92002-0.65611-1.30130-0.52000.h5\n",
      "Epoch 17/40\n",
      "34/34 [==============================] - 99s 3s/step - loss: 0.8757 - categorical_accuracy: 0.6976 - val_loss: 1.1958 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.30130 to 1.19583, saving model to model_init_2021-11-1903_30_56.460227/model-00017-0.89188-0.70287-1.19583-0.52000.h5\n",
      "Epoch 18/40\n",
      "34/34 [==============================] - 94s 3s/step - loss: 0.8078 - categorical_accuracy: 0.7110 - val_loss: 1.1309 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.19583 to 1.13089, saving model to model_init_2021-11-1903_30_56.460227/model-00018-0.74974-0.73303-1.13089-0.55000.h5\n",
      "Epoch 19/40\n",
      "34/34 [==============================] - 95s 3s/step - loss: 0.7613 - categorical_accuracy: 0.7163 - val_loss: 0.8841 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.13089 to 0.88415, saving model to model_init_2021-11-1903_30_56.460227/model-00019-0.75879-0.71192-0.88415-0.68000.h5\n",
      "Epoch 20/40\n",
      "34/34 [==============================] - 94s 3s/step - loss: 0.6979 - categorical_accuracy: 0.7203 - val_loss: 0.9923 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.88415\n",
      "Epoch 21/40\n",
      "34/34 [==============================] - 95s 3s/step - loss: 0.6928 - categorical_accuracy: 0.7358 - val_loss: 1.0786 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.88415\n",
      "Epoch 22/40\n",
      "34/34 [==============================] - 94s 3s/step - loss: 0.6486 - categorical_accuracy: 0.7486 - val_loss: 1.1261 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.88415\n",
      "Epoch 23/40\n",
      "34/34 [==============================] - 93s 3s/step - loss: 0.7229 - categorical_accuracy: 0.7289 - val_loss: 1.0578 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.88415\n",
      "Epoch 24/40\n",
      "34/34 [==============================] - 93s 3s/step - loss: 0.5716 - categorical_accuracy: 0.7704 - val_loss: 1.1427 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.88415\n",
      "Epoch 25/40\n",
      "34/34 [==============================] - 93s 3s/step - loss: 0.5456 - categorical_accuracy: 0.8004 - val_loss: 0.9781 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.88415\n",
      "Epoch 26/40\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.5646 - categorical_accuracy: 0.7775 - val_loss: 0.9191 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.88415\n",
      "Epoch 27/40\n",
      "34/34 [==============================] - 99s 3s/step - loss: 0.6542 - categorical_accuracy: 0.7589 - val_loss: 0.9928 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.88415\n",
      "Epoch 28/40\n",
      "34/34 [==============================] - 93s 3s/step - loss: 0.5572 - categorical_accuracy: 0.7892 - val_loss: 0.7489 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.88415 to 0.74889, saving model to model_init_2021-11-1903_30_56.460227/model-00028-0.61813-0.76772-0.74889-0.72000.h5\n",
      "Epoch 29/40\n",
      "34/34 [==============================] - 95s 3s/step - loss: 0.5297 - categorical_accuracy: 0.8012 - val_loss: 0.7389 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.74889 to 0.73894, saving model to model_init_2021-11-1903_30_56.460227/model-00029-0.54268-0.79035-0.73894-0.74000.h5\n",
      "Epoch 30/40\n",
      "34/34 [==============================] - 99s 3s/step - loss: 0.5978 - categorical_accuracy: 0.7763 - val_loss: 0.8307 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.73894\n",
      "Epoch 31/40\n",
      "34/34 [==============================] - 95s 3s/step - loss: 0.5427 - categorical_accuracy: 0.8137 - val_loss: 0.9416 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.73894\n",
      "Epoch 32/40\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.4328 - categorical_accuracy: 0.8365 - val_loss: 1.0175 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.73894\n",
      "Epoch 33/40\n",
      "34/34 [==============================] - 94s 3s/step - loss: 0.4678 - categorical_accuracy: 0.8145 - val_loss: 0.9820 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.73894\n",
      "Epoch 34/40\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.4876 - categorical_accuracy: 0.8183 - val_loss: 1.0520 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.73894\n",
      "Epoch 35/40\n",
      "34/34 [==============================] - 97s 3s/step - loss: 0.4293 - categorical_accuracy: 0.8209 - val_loss: 0.7657 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.73894\n",
      "Epoch 36/40\n",
      "34/34 [==============================] - 98s 3s/step - loss: 0.4906 - categorical_accuracy: 0.8288 - val_loss: 0.9970 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.73894\n",
      "Epoch 37/40\n",
      "34/34 [==============================] - 99s 3s/step - loss: 0.4770 - categorical_accuracy: 0.8183 - val_loss: 0.8940 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.73894\n",
      "Epoch 38/40\n",
      "34/34 [==============================] - 96s 3s/step - loss: 0.5212 - categorical_accuracy: 0.7889 - val_loss: 1.0683 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.73894\n",
      "Epoch 39/40\n",
      "34/34 [==============================] - 96s 3s/step - loss: 0.4686 - categorical_accuracy: 0.8161 - val_loss: 0.7376 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.73894 to 0.73762, saving model to model_init_2021-11-1903_30_56.460227/model-00039-0.46851-0.81599-0.73762-0.74000.h5\n",
      "Epoch 40/40\n",
      "34/34 [==============================] - 99s 3s/step - loss: 0.4034 - categorical_accuracy: 0.8565 - val_loss: 1.1500 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.73762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f806fe567f0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further reduce parameters by lowering down image size\n",
    "### Model 9 Batch Size: 20, Number of images from each video: 22 Image Size: 100x100, Epochs: 40 with Dropouts, two dense layer, LR=0.0002, filter size =(2,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_36 (Conv3D)           (None, 22, 100, 100, 16)  400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 22, 100, 100, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_36 (MaxPooling (None, 11, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 11, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_37 (Conv3D)           (None, 11, 50, 50, 32)    4128      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 11, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 11, 50, 50, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_37 (MaxPooling (None, 5, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 5, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_38 (Conv3D)           (None, 5, 25, 25, 64)     16448     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 5, 25, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 5, 25, 25, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_38 (MaxPooling (None, 2, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 2, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_39 (Conv3D)           (None, 2, 12, 12, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 2, 12, 12, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 2, 12, 12, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_39 (MaxPooling (None, 1, 6, 6, 128)      0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 1, 6, 6, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 695,733\n",
      "Trainable params: 694,741\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "img_per_video=22\n",
    "img_h = 100\n",
    "img_w = 100\n",
    "batch_size = 20 #experiment with the batch size\n",
    "filtersize = (2,2,2)\n",
    "num_epochs = 40 # choose the number of epochs\n",
    "img_idx_v = np.round(np.linspace(0,29,img_per_video)).astype(int)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(16, filtersize, activation='relu', input_shape=(len(img_idx_v),img_h,img_w,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(32, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(64, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(128, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train.\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam(lr=0.0002) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_idx_v, img_h, img_w)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_idx_v, img_h, img_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.001)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/40\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.5734 - categorical_accuracy: 0.2258Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 147s 4s/step - loss: 2.5702 - categorical_accuracy: 0.2267 - val_loss: 1.7067 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.70670, saving model to model_init_2021-11-1906_12_19.098061/model-00001-2.46118-0.25792-1.70670-0.21000.h5\n",
      "Epoch 2/40\n",
      "34/34 [==============================] - 130s 4s/step - loss: 2.0240 - categorical_accuracy: 0.3458 - val_loss: 2.0669 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.70670\n",
      "Epoch 3/40\n",
      "34/34 [==============================] - 130s 4s/step - loss: 1.8877 - categorical_accuracy: 0.3765 - val_loss: 2.7987 - val_categorical_accuracy: 0.1200\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.70670\n",
      "Epoch 4/40\n",
      "34/34 [==============================] - 133s 4s/step - loss: 1.7377 - categorical_accuracy: 0.3844 - val_loss: 2.9541 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.70670\n",
      "Epoch 5/40\n",
      "34/34 [==============================] - 134s 4s/step - loss: 1.5320 - categorical_accuracy: 0.4597 - val_loss: 3.4109 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.70670\n",
      "Epoch 6/40\n",
      "34/34 [==============================] - 135s 4s/step - loss: 1.5049 - categorical_accuracy: 0.4960 - val_loss: 3.3726 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.70670\n",
      "Epoch 7/40\n",
      "34/34 [==============================] - 132s 4s/step - loss: 1.5126 - categorical_accuracy: 0.4926 - val_loss: 2.8266 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.70670\n",
      "Epoch 8/40\n",
      "34/34 [==============================] - 134s 4s/step - loss: 1.4253 - categorical_accuracy: 0.4767 - val_loss: 2.8569 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.70670\n",
      "Epoch 9/40\n",
      "34/34 [==============================] - 132s 4s/step - loss: 1.4024 - categorical_accuracy: 0.4755 - val_loss: 2.5518 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.70670\n",
      "Epoch 10/40\n",
      "34/34 [==============================] - 132s 4s/step - loss: 1.3503 - categorical_accuracy: 0.4968 - val_loss: 2.3576 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.70670\n",
      "Epoch 11/40\n",
      "34/34 [==============================] - 136s 4s/step - loss: 1.3033 - categorical_accuracy: 0.5008 - val_loss: 2.1023 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.70670\n",
      "Epoch 12/40\n",
      "34/34 [==============================] - 134s 4s/step - loss: 1.3185 - categorical_accuracy: 0.5240 - val_loss: 1.9678 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.70670\n",
      "Epoch 13/40\n",
      "34/34 [==============================] - 135s 4s/step - loss: 1.1131 - categorical_accuracy: 0.5730 - val_loss: 1.9192 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.70670\n",
      "Epoch 14/40\n",
      "34/34 [==============================] - 136s 4s/step - loss: 1.1377 - categorical_accuracy: 0.5875 - val_loss: 1.6955 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.70670 to 1.69554, saving model to model_init_2021-11-1906_12_19.098061/model-00014-1.06959-0.60332-1.69554-0.44000.h5\n",
      "Epoch 15/40\n",
      "34/34 [==============================] - 137s 4s/step - loss: 1.0481 - categorical_accuracy: 0.6079 - val_loss: 1.6322 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.69554 to 1.63223, saving model to model_init_2021-11-1906_12_19.098061/model-00015-1.02222-0.61538-1.63223-0.45000.h5\n",
      "Epoch 16/40\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.9599 - categorical_accuracy: 0.6272 - val_loss: 1.6021 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.63223 to 1.60209, saving model to model_init_2021-11-1906_12_19.098061/model-00016-1.02099-0.60030-1.60209-0.45000.h5\n",
      "Epoch 17/40\n",
      "34/34 [==============================] - 132s 4s/step - loss: 1.1075 - categorical_accuracy: 0.5955 - val_loss: 1.5394 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.60209 to 1.53942, saving model to model_init_2021-11-1906_12_19.098061/model-00017-1.10135-0.59578-1.53942-0.46000.h5\n",
      "Epoch 18/40\n",
      "34/34 [==============================] - 131s 4s/step - loss: 1.0011 - categorical_accuracy: 0.6436 - val_loss: 1.4155 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.53942 to 1.41553, saving model to model_init_2021-11-1906_12_19.098061/model-00018-1.01363-0.62745-1.41553-0.48000.h5\n",
      "Epoch 19/40\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.9538 - categorical_accuracy: 0.6584 - val_loss: 1.3788 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.41553 to 1.37878, saving model to model_init_2021-11-1906_12_19.098061/model-00019-0.94992-0.65611-1.37878-0.56000.h5\n",
      "Epoch 20/40\n",
      "34/34 [==============================] - 134s 4s/step - loss: 1.0005 - categorical_accuracy: 0.6396 - val_loss: 1.3013 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.37878 to 1.30129, saving model to model_init_2021-11-1906_12_19.098061/model-00020-0.98770-0.63650-1.30129-0.55000.h5\n",
      "Epoch 21/40\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.9059 - categorical_accuracy: 0.6370 - val_loss: 1.1210 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.30129 to 1.12102, saving model to model_init_2021-11-1906_12_19.098061/model-00021-0.89761-0.64253-1.12102-0.58000.h5\n",
      "Epoch 22/40\n",
      "34/34 [==============================] - 136s 4s/step - loss: 0.9164 - categorical_accuracy: 0.6550 - val_loss: 1.1759 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.12102\n",
      "Epoch 23/40\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.9002 - categorical_accuracy: 0.6552 - val_loss: 1.1436 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.12102\n",
      "Epoch 24/40\n",
      "34/34 [==============================] - 134s 4s/step - loss: 0.7541 - categorical_accuracy: 0.7115 - val_loss: 1.0257 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.12102 to 1.02565, saving model to model_init_2021-11-1906_12_19.098061/model-00024-0.75098-0.71946-1.02565-0.66000.h5\n",
      "Epoch 25/40\n",
      "34/34 [==============================] - 136s 4s/step - loss: 0.8975 - categorical_accuracy: 0.6883 - val_loss: 1.0603 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.02565\n",
      "Epoch 26/40\n",
      "34/34 [==============================] - 135s 4s/step - loss: 0.7873 - categorical_accuracy: 0.7176 - val_loss: 0.9788 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.02565 to 0.97883, saving model to model_init_2021-11-1906_12_19.098061/model-00026-0.74799-0.73303-0.97883-0.62000.h5\n",
      "Epoch 27/40\n",
      "34/34 [==============================] - 133s 4s/step - loss: 0.7114 - categorical_accuracy: 0.7517 - val_loss: 0.9405 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.97883 to 0.94048, saving model to model_init_2021-11-1906_12_19.098061/model-00027-0.74095-0.73454-0.94048-0.64000.h5\n",
      "Epoch 28/40\n",
      "34/34 [==============================] - 133s 4s/step - loss: 0.8056 - categorical_accuracy: 0.7036 - val_loss: 1.0425 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.94048\n",
      "Epoch 29/40\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.6881 - categorical_accuracy: 0.7374 - val_loss: 0.9346 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.94048 to 0.93458, saving model to model_init_2021-11-1906_12_19.098061/model-00029-0.71536-0.72700-0.93458-0.65000.h5\n",
      "Epoch 30/40\n",
      "34/34 [==============================] - 131s 4s/step - loss: 0.6920 - categorical_accuracy: 0.7414 - val_loss: 0.9587 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.93458\n",
      "Epoch 31/40\n",
      "34/34 [==============================] - 132s 4s/step - loss: 0.6157 - categorical_accuracy: 0.7673 - val_loss: 0.9449 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.93458\n",
      "Epoch 32/40\n",
      "34/34 [==============================] - 129s 4s/step - loss: 0.6227 - categorical_accuracy: 0.7573 - val_loss: 1.0668 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.93458\n",
      "Epoch 33/40\n",
      "34/34 [==============================] - 128s 4s/step - loss: 0.5898 - categorical_accuracy: 0.7776 - val_loss: 1.2412 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.93458\n",
      "Epoch 34/40\n",
      "34/34 [==============================] - 127s 4s/step - loss: 0.6523 - categorical_accuracy: 0.7502 - val_loss: 1.2830 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.93458\n",
      "Epoch 35/40\n",
      "34/34 [==============================] - 127s 4s/step - loss: 0.6229 - categorical_accuracy: 0.7803 - val_loss: 1.0956 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.93458\n",
      "Epoch 36/40\n",
      "34/34 [==============================] - 128s 4s/step - loss: 0.5161 - categorical_accuracy: 0.7877 - val_loss: 1.0305 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.93458\n",
      "Epoch 37/40\n",
      "34/34 [==============================] - 124s 4s/step - loss: 0.5836 - categorical_accuracy: 0.7799 - val_loss: 1.0852 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.93458\n",
      "Epoch 38/40\n",
      "34/34 [==============================] - 126s 4s/step - loss: 0.5211 - categorical_accuracy: 0.7780 - val_loss: 1.1032 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.93458\n",
      "Epoch 39/40\n",
      "34/34 [==============================] - 126s 4s/step - loss: 0.5842 - categorical_accuracy: 0.7838 - val_loss: 0.9807 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.93458\n",
      "Epoch 40/40\n",
      "34/34 [==============================] - 127s 4s/step - loss: 0.4936 - categorical_accuracy: 0.8021 - val_loss: 0.9895 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.93458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f806fe554f0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 10 CNN+RNN (Transfer Learning with MobileNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "14jgqtCkIRJp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 16, 50176)         3228864   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 16)                2409312   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 5,638,261\n",
      "Trainable params: 5,616,373\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 16, 50176)         3228864   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 16)                2409312   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 5,638,261\n",
      "Trainable params: 5,616,373\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import mobilenet\n",
    "\n",
    "img_idx_v = list(np.round(np.linspace(0,29,16)).astype(int))\n",
    "\n",
    "img_h = 224\n",
    "img_w = 224\n",
    "batch_size = 15 #experiment with the batch size\n",
    "num_epochs=25\n",
    "\n",
    "base_model=mobilenet.MobileNet(input_shape=(img_h, img_w, 3), weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "x = Flatten()(base_model.output)\n",
    "mob_net = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(mob_net, input_shape=(len(img_idx_v), img_h, img_w,3)))\n",
    "model.add(GRU(len(img_idx_v)))\n",
    "model.add(Dropout(.25)) #added\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam(lr=0.0002) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2X2oVZXTPSMi"
   },
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_idx_v, img_h, img_w)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_idx_v, img_h, img_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-MsBuB2PSMi",
    "outputId": "0da2a3f8-c662-4fa5-9a24-ad506fb16c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.001)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uvgcA1PqPSMj"
   },
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3dNAY5bPSMj",
    "outputId": "0f391e1b-f1f8-447a-96d1-8b88619dfc3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/envs/cuda101/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 15\n",
      "Epoch 1/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.6327 - categorical_accuracy: 0.2947Source path =  Project_data/val ; batch size = 15\n",
      "45/45 [==============================] - 238s 4s/step - loss: 1.6285 - categorical_accuracy: 0.2961 - val_loss: 1.2138 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21376, saving model to model_init_2021-11-1915_36_34.884718/model-00001-1.43848-0.35897-1.21376-0.53000.h5\n",
      "Epoch 2/25\n",
      "45/45 [==============================] - 139s 3s/step - loss: 1.0908 - categorical_accuracy: 0.5642 - val_loss: 0.9284 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21376 to 0.92839, saving model to model_init_2021-11-1915_36_34.884718/model-00002-1.07321-0.53846-0.92839-0.65000.h5\n",
      "Epoch 3/25\n",
      "45/45 [==============================] - 94s 2s/step - loss: 0.9914 - categorical_accuracy: 0.5684 - val_loss: 1.0091 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.92839\n",
      "Epoch 4/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.9004 - categorical_accuracy: 0.6683 - val_loss: 0.8086 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.92839 to 0.80862, saving model to model_init_2021-11-1915_36_34.884718/model-00004-0.89601-0.64103-0.80862-0.71000.h5\n",
      "Epoch 5/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.8482 - categorical_accuracy: 0.6321 - val_loss: 0.7591 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.80862 to 0.75908, saving model to model_init_2021-11-1915_36_34.884718/model-00005-0.82124-0.66365-0.75908-0.72000.h5\n",
      "Epoch 6/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.7793 - categorical_accuracy: 0.6785 - val_loss: 0.7213 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.75908 to 0.72133, saving model to model_init_2021-11-1915_36_34.884718/model-00006-0.78187-0.66516-0.72133-0.71000.h5\n",
      "Epoch 7/25\n",
      "45/45 [==============================] - 89s 2s/step - loss: 0.7729 - categorical_accuracy: 0.6715 - val_loss: 0.7522 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.72133\n",
      "Epoch 8/25\n",
      "45/45 [==============================] - 89s 2s/step - loss: 0.7556 - categorical_accuracy: 0.7733 - val_loss: 0.7655 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.72133\n",
      "Epoch 9/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.7021 - categorical_accuracy: 0.7681 - val_loss: 0.7669 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.72133\n",
      "Epoch 10/25\n",
      "45/45 [==============================] - 90s 2s/step - loss: 0.6255 - categorical_accuracy: 0.8000 - val_loss: 0.6525 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.72133 to 0.65247, saving model to model_init_2021-11-1915_36_34.884718/model-00010-0.63667-0.80543-0.65247-0.87000.h5\n",
      "Epoch 11/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.6191 - categorical_accuracy: 0.8412 - val_loss: 0.5781 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.65247 to 0.57805, saving model to model_init_2021-11-1915_36_34.884718/model-00011-0.61824-0.85219-0.57805-0.87000.h5\n",
      "Epoch 12/25\n",
      "45/45 [==============================] - 89s 2s/step - loss: 0.6100 - categorical_accuracy: 0.8565 - val_loss: 0.5701 - val_categorical_accuracy: 0.9100\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.57805 to 0.57013, saving model to model_init_2021-11-1915_36_34.884718/model-00012-0.59716-0.85973-0.57013-0.91000.h5\n",
      "Epoch 13/25\n",
      "45/45 [==============================] - 95s 2s/step - loss: 0.6140 - categorical_accuracy: 0.8540 - val_loss: 0.5815 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.57013\n",
      "Epoch 14/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.5941 - categorical_accuracy: 0.8706 - val_loss: 0.5317 - val_categorical_accuracy: 0.8900\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.57013 to 0.53170, saving model to model_init_2021-11-1915_36_34.884718/model-00014-0.57283-0.87632-0.53170-0.89000.h5\n",
      "Epoch 15/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.5317 - categorical_accuracy: 0.9127 - val_loss: 0.5678 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.53170\n",
      "Epoch 16/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.5569 - categorical_accuracy: 0.9181 - val_loss: 0.6221 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.53170\n",
      "Epoch 17/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.5944 - categorical_accuracy: 0.8682 - val_loss: 0.5014 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.53170 to 0.50145, saving model to model_init_2021-11-1915_36_34.884718/model-00017-0.57352-0.87632-0.50145-0.90000.h5\n",
      "Epoch 18/25\n",
      "45/45 [==============================] - 87s 2s/step - loss: 0.5684 - categorical_accuracy: 0.9041 - val_loss: 0.5288 - val_categorical_accuracy: 0.8900\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.50145\n",
      "Epoch 19/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.5025 - categorical_accuracy: 0.9092 - val_loss: 0.5783 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.50145\n",
      "Epoch 20/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.4835 - categorical_accuracy: 0.9208 - val_loss: 0.5152 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.50145\n",
      "Epoch 21/25\n",
      "45/45 [==============================] - 87s 2s/step - loss: 0.4765 - categorical_accuracy: 0.9048 - val_loss: 0.4863 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.50145 to 0.48628, saving model to model_init_2021-11-1915_36_34.884718/model-00021-0.44336-0.92459-0.48628-0.87000.h5\n",
      "Epoch 22/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.4205 - categorical_accuracy: 0.9347 - val_loss: 0.5150 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48628\n",
      "Epoch 23/25\n",
      "45/45 [==============================] - 89s 2s/step - loss: 0.3968 - categorical_accuracy: 0.9590 - val_loss: 0.4838 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.48628 to 0.48383, saving model to model_init_2021-11-1915_36_34.884718/model-00023-0.39795-0.95173-0.48383-0.88000.h5\n",
      "Epoch 24/25\n",
      "45/45 [==============================] - 88s 2s/step - loss: 0.4157 - categorical_accuracy: 0.9211 - val_loss: 0.4932 - val_categorical_accuracy: 0.8900\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48383\n",
      "Epoch 25/25\n",
      "45/45 [==============================] - 87s 2s/step - loss: 0.4304 - categorical_accuracy: 0.9031 - val_loss: 0.4054 - val_categorical_accuracy: 0.9200\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.48383 to 0.40541, saving model to model_init_2021-11-1915_36_34.884718/model-00025-0.43445-0.91101-0.40541-0.92000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f31019d4ee0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 11 CNN+RNN (Without Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6JueECQtpTnY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_13 (TimeDis (None, 22, 120, 120, 16)  208       \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 22, 120, 120, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 22, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 22, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 22, 60, 60, 32)    2080      \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 22, 60, 60, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 22, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 22, 30, 30, 64)    8256      \n",
      "_________________________________________________________________\n",
      "time_distributed_20 (TimeDis (None, 22, 30, 30, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 22, 15, 15, 64)    0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 22, 15, 15, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 22, 15, 15, 128)   32896     \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 22, 15, 15, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 22, 7, 7, 128)     0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 22, 7, 7, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 22, 6272)          0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64)                1216896   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,270,261\n",
      "Trainable params: 1,269,781\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "img_idx_v = list(np.round(np.linspace(0,29,22)).astype(int))\n",
    "\n",
    "img_h = 120\n",
    "img_w = 120\n",
    "batch_size = 20 #experiment with the batch size\n",
    "num_epochs=25\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(16, (2, 2) , padding='same', activation='relu'), input_shape=(len(img_idx_v), img_h, img_w,3)))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(Dropout(0.1))\n",
    "        \n",
    "model.add(TimeDistributed(Conv2D(32, (2, 2) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "model.add(TimeDistributed(Conv2D(64, (2, 2) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(Dropout(0.1))\n",
    "        \n",
    "model.add(TimeDistributed(Conv2D(128, (2, 2) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(GRU(64))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_idx_v, img_h, img_w)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_idx_v, img_h, img_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.001)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/envs/cuda101/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.7298 - categorical_accuracy: 0.2232Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 151s 4s/step - loss: 1.7263 - categorical_accuracy: 0.2246 - val_loss: 1.6678 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.66784, saving model to model_init_2021-11-2001_45_15.111756/model-00001-1.60621-0.27149-1.66784-0.18000.h5\n",
      "Epoch 2/25\n",
      "34/34 [==============================] - 66s 2s/step - loss: 1.3350 - categorical_accuracy: 0.4402 - val_loss: 1.6230 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.66784 to 1.62300, saving model to model_init_2021-11-2001_45_15.111756/model-00002-1.32690-0.45098-1.62300-0.22000.h5\n",
      "Epoch 3/25\n",
      "34/34 [==============================] - 57s 2s/step - loss: 1.2764 - categorical_accuracy: 0.4730 - val_loss: 1.6237 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.62300\n",
      "Epoch 4/25\n",
      "34/34 [==============================] - 56s 2s/step - loss: 1.1703 - categorical_accuracy: 0.4928 - val_loss: 1.6462 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.62300\n",
      "Epoch 5/25\n",
      "34/34 [==============================] - 57s 2s/step - loss: 1.0597 - categorical_accuracy: 0.5551 - val_loss: 1.7068 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.62300\n",
      "Epoch 6/25\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.9318 - categorical_accuracy: 0.6476 - val_loss: 1.8207 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.62300\n",
      "Epoch 7/25\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.8577 - categorical_accuracy: 0.6711 - val_loss: 2.0113 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.62300\n",
      "Epoch 8/25\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.6854 - categorical_accuracy: 0.7408 - val_loss: 2.0264 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.62300\n",
      "Epoch 9/25\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.6475 - categorical_accuracy: 0.7689 - val_loss: 2.0084 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.62300\n",
      "Epoch 10/25\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.6249 - categorical_accuracy: 0.7503 - val_loss: 2.6827 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.62300\n",
      "Epoch 11/25\n",
      "34/34 [==============================] - 56s 2s/step - loss: 0.5883 - categorical_accuracy: 0.7727 - val_loss: 1.8297 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.62300\n",
      "Epoch 12/25\n",
      "34/34 [==============================] - 56s 2s/step - loss: 0.5075 - categorical_accuracy: 0.8069 - val_loss: 2.9933 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.62300\n",
      "Epoch 13/25\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.5086 - categorical_accuracy: 0.7914 - val_loss: 1.6597 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.62300\n",
      "Epoch 14/25\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.4631 - categorical_accuracy: 0.8512 - val_loss: 1.5235 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.62300 to 1.52348, saving model to model_init_2021-11-2001_45_15.111756/model-00014-0.48055-0.82504-1.52348-0.47000.h5\n",
      "Epoch 15/25\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.5696 - categorical_accuracy: 0.7908 - val_loss: 1.4411 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.52348 to 1.44106, saving model to model_init_2021-11-2001_45_15.111756/model-00015-0.51687-0.81297-1.44106-0.52000.h5\n",
      "Epoch 16/25\n",
      "34/34 [==============================] - 56s 2s/step - loss: 0.4232 - categorical_accuracy: 0.8415 - val_loss: 1.4743 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.44106\n",
      "Epoch 17/25\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.4223 - categorical_accuracy: 0.8403 - val_loss: 1.2841 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.44106 to 1.28414, saving model to model_init_2021-11-2001_45_15.111756/model-00017-0.43897-0.82051-1.28414-0.58000.h5\n",
      "Epoch 18/25\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.3909 - categorical_accuracy: 0.8657 - val_loss: 0.6218 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.28414 to 0.62183, saving model to model_init_2021-11-2001_45_15.111756/model-00018-0.42096-0.84615-0.62183-0.81000.h5\n",
      "Epoch 19/25\n",
      "34/34 [==============================] - 56s 2s/step - loss: 0.3854 - categorical_accuracy: 0.8659 - val_loss: 1.3392 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.62183\n",
      "Epoch 20/25\n",
      "34/34 [==============================] - 56s 2s/step - loss: 0.3569 - categorical_accuracy: 0.8826 - val_loss: 0.8696 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.62183\n",
      "Epoch 21/25\n",
      "34/34 [==============================] - 56s 2s/step - loss: 0.2932 - categorical_accuracy: 0.8892 - val_loss: 0.8742 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.62183\n",
      "Epoch 22/25\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.3207 - categorical_accuracy: 0.8715 - val_loss: 0.9016 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.62183\n",
      "Epoch 23/25\n",
      "34/34 [==============================] - 56s 2s/step - loss: 0.2968 - categorical_accuracy: 0.8949 - val_loss: 0.8318 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.62183\n",
      "Epoch 24/25\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.2350 - categorical_accuracy: 0.9192 - val_loss: 0.7124 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.62183\n",
      "Epoch 25/25\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.1658 - categorical_accuracy: 0.9422 - val_loss: 0.8053 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.62183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f393c0562b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "With two architecture approaches where we have Conv3D and CNN+RNN, we had multiple experiments. Though we have shown multiple experiments with Conv3D in this notebook, we kept only 2 models with CNN+RNN giving pretty good performance to keep the notebook simlified.\n",
    "\n",
    "We have now Two models with pretty good results and optimized parameters.\n",
    "\n",
    "#### Model 8: Conv3D, Trainable params: 907,733, Training Accuracy--80.12%, Validation Accuracy--74%\n",
    "\n",
    "### Model 11: CNN+RNN( without Transfer Learning), Trainable params: 1,269,781, Training Accuracy--86.57%, Validation Accuracy--81%\n",
    "\n",
    "**Now since evaluation is based on number of parameters + accuracy of model we decided to submit Model 11 h5 file where though number of parameters a bit high compared to Model 8 but results of Model 11 are significantly better\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural_Nets_Project_Starter_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
